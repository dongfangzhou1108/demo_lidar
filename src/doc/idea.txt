初始化：
	紧耦合估计深度的话，ceres不收敛；
	感觉在归一化平面不如在像素上寻找，但是投影的速度太慢且本质相同；
	自动驾驶大尺度，因此筛选应该更加严格；
	我们跟踪的总是角点，那么激光雷达在相关地方也很难形成平面？
	主要问题是有深度的特征少，而且深度不稳定连续
	重点是怎么紧耦合，也许可以多找几个点来估计深度增加鲁棒性？

	震惊，初始化过程中的三维点居然有一堆零，因为这些点都是以第一帧为参考系的，
		１、所以让他们为零，等于没有优化这些点？去vins求证。已经证实
		２、而且其中有部分点的位置及其离谱，例如
		[ INFO] [1614298176.944339423]: p with depth: -12.799817, -0.893252, 29.855392; p without depth: -589.167858, -43.067435, 1370.344822
			原因在于，只要这个点距离足够远，那么我们在优化过程中，传感器的运动收到该点的影响就会比较小，因此这种情况也是可以接受的？
		在赋予深度的过程中，应该考虑阈值，比如30米以内
		除此之外，三角化的点可能会有一些深度过小(<0.1)的点存在，等待优化
		为什么三角化需要：it_per_id.start_frame < WINDOW_SIZE - 2：滑动窗口时没有计算新一帧的位姿
		除此之外：vins的非线性优化对于ceres来说同样不收敛，为啥应用逆深度这个VO不收敛啊(刚开始还是收敛的，后来就不收敛了，不用狗腿直接不好使)
			因为单目情况，尺度不能保持，所以不好使，优化之后尺度就变了；
		针对这种尺度不客观性能，我们选择固定收尾两帧，但这样可能不太合适
			问题存在于，如果最新一帧的位姿PnP不准确，算法也会不好使；
		实验证明，获取深度点的里程计PnP本身的精度就非常低，毫无优势

	或许应该尝试松耦合求解尺度？
	需要担心的就是深度点过少，导致PnP求解不够准确
	但是，在该过程中，我们仅仅利用激光雷达赋予的深度来赋予相机运动的尺度，这样做相比于利用imu或者GPS是没有任何优势的

总结：
初始化过程中：思路是通过寻找归一化平面距离比较接近的激光雷达点来赋予相机平面的深度
	首先，可以考虑投影和归一化平面：这一部分本质相同，但是全部投影过去会大大增加时间，pass；
	其次，筛选思路的确定：根据找到的(1)三个点和其(2)投影像素的协方差以及(3)计算的深度差决定；
			筛选细节增加：防止点都在一侧
		我们的本来思路是通过计算三角化的点以及深度点的比例，来计算单目里程计的深度，
		但是根据ceres计算的三角化的点不仅有很大一部分零点，还有一部分远的离谱的点，这样就会导致尺度计算的一定困难；
		根据深度点位姿PnP计算，这样的话有点相当于松耦合？